#!/usr/bin/env python3
"""
Script de Validation Finale - IntÃ©gration Frame.io
Test de tous les composants essentiels sans configuration externe
"""

import sys
import os
from pathlib import Path
import tempfile
import json
from datetime import datetime

# Ajouter le chemin source
sys.path.insert(0, str(Path(__file__).parent.parent))

def print_header(title):
    """Affiche un en-tÃªte formatÃ©"""
    print(f"\n{'='*60}")
    print(f"ğŸ¬ {title}")
    print(f"{'='*60}")

def print_section(title):
    """Affiche une section formatÃ©e"""
    print(f"\n{'ğŸ” ' + title}")
    print("-" * 50)

def test_module_structure():
    """Teste la structure des modules"""
    print_section("STRUCTURE DES MODULES")
    
    modules_to_test = [
        ("Parser", "src.integrations.frameio.parser", "FrameIOFileParser"),
        ("Structure", "src.integrations.frameio.structure", "FrameIOStructureManager"),
        ("Upload", "src.integrations.frameio.upload", "FrameIOUploadManager"),
        ("Notifier", "src.integrations.frameio.notifier", "FrameIODiscordNotifier"),
        ("Integration", "src.integrations.frameio.integration", "FrameIOIntegrationManager"),
        ("OAuth Auth", "src.integrations.frameio.oauth_auth", "FrameIOOAuthAuth"),
        ("Bridge", "src.workflows.lucidlink_frameio_bridge", "LucidLinkFrameIOBridge")
    ]
    
    results = []
    for name, module_path, class_name in modules_to_test:
        try:
            module = __import__(module_path, fromlist=[class_name])
            cls = getattr(module, class_name)
            print(f"  âœ… {name}: {class_name}")
            results.append(True)
        except Exception as e:
            print(f"  âŒ {name}: {e}")
            results.append(False)
    
    print(f"\nğŸ“Š RÃ©sultat: {sum(results)}/{len(results)} modules OK")
    return all(results)

def test_parser_comprehensive():
    """Test complet du parser"""
    print_section("PARSER COMPLET")
    
    try:
        from src.integrations.frameio.parser import FrameIOFileParser
        parser = FrameIOFileParser()
        
        # Test cases avec diffÃ©rents formats
        test_cases = [
            # Format court
            ("SC01_UNDLM_00412_V002.mov", "SC01", "UNDLM_00412", "V002"),
            ("SC10_UNDLM_00523_V001.mp4", "SC10", "UNDLM_00523", "V001"),
            
            # Format long
            ("13H49_-_RECUPERATION_DES_2_SURVIVANTS_S01_V001.mov", "13H49_RECUPERATION_DES_2_SURVIVANTS", "S01", "V001"),
            ("SCENE_BATAILLE_FINALE_S05_V003.r3d", "SCENE_BATAILLE_FINALE", "S05", "V003"),
            
            # Format gÃ©nÃ©rique
            ("SCENE_42_SHOT_001_V003.r3d", "SCENE_42", "SHOT_001", "V003"),
            ("SCENE_1_SHOT_010_V001.braw", "SCENE_1", "SHOT_010", "V001"),
            
            # Extensions diverses
            ("SC01_UNDLM_00412_V002.prores", "SC01", "UNDLM_00412", "V002"),
            ("SC01_UNDLM_00412_V002.dnxhd", "SC01", "UNDLM_00412", "V002"),
        ]
        
        results = []
        for filename, expected_scene, expected_shot, expected_version in test_cases:
            metadata = parser.parse_filename(filename)
            if metadata:
                if (metadata.scene_name == expected_scene and 
                    metadata.shot_id == expected_shot and 
                    metadata.version == expected_version):
                    print(f"  âœ… {filename}")
                    print(f"      â†’ {metadata.nomenclature}")
                    results.append(True)
                else:
                    print(f"  âŒ {filename} - RÃ©sultat incorrect")
                    print(f"      Attendu: {expected_scene}_{expected_shot}_{expected_version}")
                    print(f"      Obtenu:  {metadata.scene_name}_{metadata.shot_id}_{metadata.version}")
                    results.append(False)
            else:
                print(f"  âŒ {filename} - Parse failed")
                results.append(False)
        
        # Test fichiers invalides
        invalid_files = [
            "fichier_sans_pattern.mov",
            "INVALID_FILE.txt",
            "SC01_UNDLM_INVALID.mov"
        ]
        
        print(f"\n  ğŸ” Test fichiers invalides:")
        for filename in invalid_files:
            metadata = parser.parse_filename(filename)
            if metadata is None:
                print(f"  âœ… {filename} - RejetÃ© comme attendu")
                results.append(True)
            else:
                print(f"  âŒ {filename} - Parse inattendu")
                results.append(False)
        
        print(f"\nğŸ“Š RÃ©sultat: {sum(results)}/{len(results)} tests OK")
        return all(results)
        
    except Exception as e:
        print(f"âŒ Erreur du test parser: {e}")
        return False

def test_configuration_files():
    """Test des fichiers de configuration"""
    print_section("FICHIERS DE CONFIGURATION")
    
    config_files = [
        ("config/frameio_integration.json.example", "Configuration principale"),
        ("config/frameio_structure.json", "Cache de structure"),
        ("config/frameio_config.json", "Configuration Frame.io"),
        ("config/integrations.json.example", "Configuration intÃ©grations"),
        ("config/error_handling.json", "Gestion d'erreurs")
    ]
    
    results = []
    for file_path, description in config_files:
        path = Path(file_path)
        if path.exists():
            try:
                with open(path, 'r') as f:
                    config = json.load(f)
                print(f"  âœ… {description}: {file_path}")
                results.append(True)
            except json.JSONDecodeError as e:
                print(f"  âŒ {description}: JSON invalide - {e}")
                results.append(False)
        else:
            print(f"  âš ï¸  {description}: {file_path} - Non trouvÃ©")
            results.append(False)
    
    print(f"\nğŸ“Š RÃ©sultat: {sum(results)}/{len(results)} configurations OK")
    return sum(results) >= len(results) * 0.8  # 80% minimum

def test_project_completeness():
    """Test de complÃ©tude du projet"""
    print_section("COMPLÃ‰TUDE DU PROJET")
    
    required_items = [
        # Dossiers principaux
        ("src/integrations/frameio/", "Modules Frame.io"),
        ("src/workflows/", "Workflows"),
        ("scripts/", "Scripts utilitaires"),
        ("tests/", "Tests"),
        ("docs/", "Documentation"),
        ("config/", "Configuration"),
        ("examples/", "Exemples"),
        
        # Fichiers clÃ©s
        ("src/integrations/frameio/__init__.py", "Module init"),
        ("scripts/test_frameio_quick.py", "Test rapide"),
        ("scripts/test_frameio_interactive.py", "Test interactif"),
        ("scripts/deploy_frameio_integration.sh", "Script dÃ©ploiement"),
        ("scripts/monitor_frameio_integration.py", "Monitoring"),
        ("tests/test_frameio_integration.py", "Tests unitaires"),
        ("docs/FRAMEIO_TESTING_GUIDE.md", "Guide de tests"),
        ("docs/FRAMEIO_LUCIDLINK_INTEGRATION.md", "Documentation intÃ©gration"),
        ("README_FRAMEIO_INTEGRATION.md", "README intÃ©gration"),
        ("systemd/frameio-bridge.service", "Service systemd")
    ]
    
    results = []
    for item_path, description in required_items:
        path = Path(item_path)
        if path.exists():
            print(f"  âœ… {description}: {item_path}")
            results.append(True)
        else:
            print(f"  âŒ {description}: {item_path} - Manquant")
            results.append(False)
    
    print(f"\nğŸ“Š RÃ©sultat: {sum(results)}/{len(results)} Ã©lÃ©ments prÃ©sents")
    return all(results)

def test_documentation():
    """Test de la documentation"""
    print_section("DOCUMENTATION")
    
    docs_to_check = [
        ("docs/FRAMEIO_TESTING_GUIDE.md", "Guide de tests"),
        ("docs/FRAMEIO_LUCIDLINK_INTEGRATION.md", "Documentation intÃ©gration"),
        ("README_FRAMEIO_INTEGRATION.md", "README principal"),
        ("FRAMEIO_TESTING_SUMMARY.md", "RÃ©sumÃ© des tests")
    ]
    
    results = []
    for doc_path, description in docs_to_check:
        path = Path(doc_path)
        if path.exists():
            try:
                with open(path, 'r') as f:
                    content = f.read()
                    if len(content) > 100:  # Au moins 100 caractÃ¨res
                        print(f"  âœ… {description}: {len(content)} caractÃ¨res")
                        results.append(True)
                    else:
                        print(f"  âš ï¸  {description}: Contenu trop court")
                        results.append(False)
            except Exception as e:
                print(f"  âŒ {description}: Erreur lecture - {e}")
                results.append(False)
        else:
            print(f"  âŒ {description}: Non trouvÃ©")
            results.append(False)
    
    print(f"\nğŸ“Š RÃ©sultat: {sum(results)}/{len(results)} documentations OK")
    return all(results)

def test_examples():
    """Test des exemples"""
    print_section("EXEMPLES")
    
    example_files = [
        ("examples/frameio_lucidlink_demo.py", "DÃ©mo principale"),
        ("examples/frameio_usage_examples.py", "Exemples d'usage"),
        ("examples/pipeline_demo.py", "DÃ©mo pipeline")
    ]
    
    results = []
    for example_path, description in example_files:
        path = Path(example_path)
        if path.exists():
            try:
                with open(path, 'r') as f:
                    content = f.read()
                    if 'def' in content or 'class' in content:
                        print(f"  âœ… {description}: Code Python dÃ©tectÃ©")
                        results.append(True)
                    else:
                        print(f"  âš ï¸  {description}: Pas de code Python dÃ©tectÃ©")
                        results.append(False)
            except Exception as e:
                print(f"  âŒ {description}: Erreur lecture - {e}")
                results.append(False)
        else:
            print(f"  âŒ {description}: Non trouvÃ©")
            results.append(False)
    
    print(f"\nğŸ“Š RÃ©sultat: {sum(results)}/{len(results)} exemples OK")
    return all(results)

def generate_final_report():
    """GÃ©nÃ¨re un rapport final"""
    print_section("RAPPORT FINAL")
    
    # ExÃ©cuter tous les tests
    test_results = {
        "Structure des modules": test_module_structure(),
        "Parser complet": test_parser_comprehensive(),
        "Configuration": test_configuration_files(),
        "ComplÃ©tude du projet": test_project_completeness(),
        "Documentation": test_documentation(),
        "Exemples": test_examples()
    }
    
    # Calculer les statistiques
    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results.values() if result)
    success_rate = (passed_tests / total_tests) * 100
    
    print(f"\nğŸ“Š RÃ‰SULTATS FINAUX:")
    print("-" * 30)
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {test_name}")
    
    print("-" * 30)
    print(f"ğŸ“ˆ Taux de rÃ©ussite: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
    
    if success_rate >= 90:
        print("ğŸ‰ EXCELLENT! L'intÃ©gration Frame.io est prÃªte pour la production.")
        status = "EXCELLENT"
    elif success_rate >= 75:
        print("âœ… BIEN! L'intÃ©gration Frame.io est fonctionnelle avec quelques amÃ©liorations possibles.")
        status = "BIEN"
    elif success_rate >= 60:
        print("âš ï¸  ACCEPTABLE! L'intÃ©gration Frame.io fonctionne mais nÃ©cessite des corrections.")
        status = "ACCEPTABLE"
    else:
        print("âŒ PROBLÃ‰MATIQUE! L'intÃ©gration Frame.io nÃ©cessite des corrections importantes.")
        status = "PROBLÃ‰MATIQUE"
    
    # Sauvegarder le rapport
    report = {
        "timestamp": datetime.now().isoformat(),
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "success_rate": success_rate,
        "status": status,
        "test_results": test_results
    }
    
    report_path = Path("output/frameio_validation_report.json")
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ“„ Rapport sauvegardÃ©: {report_path}")
    
    return success_rate >= 75

def main():
    """Fonction principale"""
    print_header("VALIDATION FINALE - INTÃ‰GRATION FRAME.IO")
    
    print("ğŸš€ DÃ©but de la validation complÃ¨te...")
    print("Cette validation teste tous les composants essentiels")
    print("sans nÃ©cessiter de configuration externe.")
    
    success = generate_final_report()
    
    print_section("PROCHAINES Ã‰TAPES")
    
    if success:
        print("âœ… L'intÃ©gration Frame.io est validÃ©e!")
        print("\nğŸ¯ Prochaines Ã©tapes recommandÃ©es:")
        print("1. Configurer les tokens Frame.io et Discord")
        print("2. ExÃ©cuter les tests de connectivitÃ©:")
        print("   python scripts/test_frameio_interactive.py")
        print("3. Tester avec de vrais fichiers:")
        print("   python examples/frameio_lucidlink_demo.py")
        print("4. DÃ©ployer en production:")
        print("   bash scripts/deploy_frameio_integration.sh")
    else:
        print("âŒ La validation a dÃ©tectÃ© des problÃ¨mes.")
        print("\nğŸ”§ Actions recommandÃ©es:")
        print("1. VÃ©rifier les erreurs ci-dessus")
        print("2. Corriger les fichiers manquants")
        print("3. Re-exÃ©cuter la validation")
        print("4. Consulter la documentation:")
        print("   cat docs/FRAMEIO_TESTING_GUIDE.md")
    
    print(f"\nğŸ“‹ Voir le rÃ©sumÃ© complet: FRAMEIO_TESTING_SUMMARY.md")
    print(f"ğŸ“Š Rapport dÃ©taillÃ©: output/frameio_validation_report.json")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
