#!/usr/bin/env python3
"""
Script de Validation Finale - Int√©gration Frame.io
Test de tous les composants essentiels sans configuration externe
"""

import sys
import os
from pathlib import Path
import tempfile
import json
from datetime import datetime

# Ajouter le chemin source
sys.path.insert(0, str(Path(__file__).parent.parent))

def print_header(title):
    """Affiche un en-t√™te format√©"""
    print(f"\n{'='*60}")
    print(f"üé¨ {title}")
    print(f"{'='*60}")

def print_section(title):
    """Affiche une section format√©e"""
    print(f"\n{'üîç ' + title}")
    print("-" * 50)

def test_module_structure():
    """Teste la structure des modules"""
    print_section("STRUCTURE DES MODULES")
    
    modules_to_test = [
        ("Parser", "src.integrations.frameio.parser", "FrameIOFileParser"),
        ("Structure", "src.integrations.frameio.structure", "FrameIOStructureManager"),
        ("Upload", "src.integrations.frameio.upload", "FrameIOUploadManager"),
        ("Notifier", "src.integrations.frameio.notifier", "FrameIODiscordNotifier"),
        ("Integration", "src.integrations.frameio.integration", "FrameIOIntegrationManager"),
        ("OAuth Auth", "src.integrations.frameio.oauth_auth", "FrameIOOAuthAuth"),
        ("Bridge", "src.workflows.lucidlink_frameio_bridge", "LucidLinkFrameIOBridge")
    ]
    
    results = []
    for name, module_path, class_name in modules_to_test:
        try:
            module = __import__(module_path, fromlist=[class_name])
            cls = getattr(module, class_name)
            print(f"  ‚úÖ {name}: {class_name}")
            results.append(True)
        except Exception as e:
            print(f"  ‚ùå {name}: {e}")
            results.append(False)
    
    print(f"\nüìä R√©sultat: {sum(results)}/{len(results)} modules OK")
    return all(results)

def test_parser_comprehensive():
    """Test complet du parser"""
    print_section("PARSER COMPLET")
    
    try:
        from src.integrations.frameio.parser import FrameIOFileParser
        parser = FrameIOFileParser()
        
        # Test cases avec diff√©rents formats
        test_cases = [
            # Format court
            ("SC01_UNDLM_00412_V002.mov", "SC01", "UNDLM_00412", "V002"),
            ("SC10_UNDLM_00523_V001.mp4", "SC10", "UNDLM_00523", "V001"),
            
            # Format long
            ("13H49_-_RECUPERATION_DES_2_SURVIVANTS_S01_V001.mov", "13H49_RECUPERATION_DES_2_SURVIVANTS", "S01", "V001"),
            ("SCENE_BATAILLE_FINALE_S05_V003.r3d", "SCENE_BATAILLE_FINALE", "S05", "V003"),
            
            # Format g√©n√©rique
            ("SCENE_42_SHOT_001_V003.r3d", "SCENE_42", "SHOT_001", "V003"),
            ("SCENE_1_SHOT_010_V001.braw", "SCENE_1", "SHOT_010", "V001"),
            
            # Extensions diverses
            ("SC01_UNDLM_00412_V002.prores", "SC01", "UNDLM_00412", "V002"),
            ("SC01_UNDLM_00412_V002.dnxhd", "SC01", "UNDLM_00412", "V002"),
        ]
        
        results = []
        for filename, expected_scene, expected_shot, expected_version in test_cases:
            metadata = parser.parse_filename(filename)
            if metadata:
                if (metadata.scene_name == expected_scene and 
                    metadata.shot_id == expected_shot and 
                    metadata.version == expected_version):
                    print(f"  ‚úÖ {filename}")
                    print(f"      ‚Üí {metadata.nomenclature}")
                    results.append(True)
                else:
                    print(f"  ‚ùå {filename} - R√©sultat incorrect")
                    print(f"      Attendu: {expected_scene}_{expected_shot}_{expected_version}")
                    print(f"      Obtenu:  {metadata.scene_name}_{metadata.shot_id}_{metadata.version}")
                    results.append(False)
            else:
                print(f"  ‚ùå {filename} - Parse failed")
                results.append(False)
        
        # Test fichiers invalides
        invalid_files = [
            "fichier_sans_pattern.mov",
            "INVALID_FILE.txt",
            "SC01_UNDLM_INVALID.mov"
        ]
        
        print(f"\n  üîç Test fichiers invalides:")
        for filename in invalid_files:
            metadata = parser.parse_filename(filename)
            if metadata is None:
                print(f"  ‚úÖ {filename} - Rejet√© comme attendu")
                results.append(True)
            else:
                print(f"  ‚ùå {filename} - Parse inattendu")
                results.append(False)
        
        print(f"\nüìä R√©sultat: {sum(results)}/{len(results)} tests OK")
        return all(results)
        
    except Exception as e:
        print(f"‚ùå Erreur du test parser: {e}")
        return False

def test_configuration_files():
    """Test des fichiers de configuration"""
    print_section("FICHIERS DE CONFIGURATION")
    
    config_files = [
        ("config/frameio_integration.json.example", "Configuration principale"),
        ("config/frameio_structure.json", "Cache de structure"),
        ("config/frameio_config.json", "Configuration Frame.io"),
        ("config/integrations.json.example", "Configuration int√©grations"),
        ("config/error_handling.json", "Gestion d'erreurs")
    ]
    
    results = []
    for file_path, description in config_files:
        path = Path(file_path)
        if path.exists():
            try:
                with open(path, 'r') as f:
                    config = json.load(f)
                print(f"  ‚úÖ {description}: {file_path}")
                results.append(True)
            except json.JSONDecodeError as e:
                print(f"  ‚ùå {description}: JSON invalide - {e}")
                results.append(False)
        else:
            print(f"  ‚ö†Ô∏è  {description}: {file_path} - Non trouv√©")
            results.append(False)
    
    print(f"\nüìä R√©sultat: {sum(results)}/{len(results)} configurations OK")
    return sum(results) >= len(results) * 0.8  # 80% minimum

def test_project_completeness():
    """Test de compl√©tude du projet"""
    print_section("COMPL√âTUDE DU PROJET")
    
    required_items = [
        # Dossiers principaux
        ("src/integrations/frameio/", "Modules Frame.io"),
        ("src/workflows/", "Workflows"),
        ("scripts/", "Scripts utilitaires"),
        ("tests/", "Tests"),
        ("docs/", "Documentation"),
        ("config/", "Configuration"),
        ("examples/", "Exemples"),
        
        # Fichiers cl√©s
        ("src/integrations/frameio/__init__.py", "Module init"),
        ("scripts/test_frameio_quick.py", "Test rapide"),
        ("scripts/test_frameio_interactive.py", "Test interactif"),
        ("scripts/deploy_frameio_integration.sh", "Script d√©ploiement"),
        ("scripts/monitor_frameio_integration.py", "Monitoring"),
        ("tests/test_frameio_integration.py", "Tests unitaires"),
        ("docs/FRAMEIO_TESTING_GUIDE.md", "Guide de tests"),
        ("docs/FRAMEIO_LUCIDLINK_INTEGRATION.md", "Documentation int√©gration"),
        ("README_FRAMEIO_INTEGRATION.md", "README int√©gration"),
        ("systemd/frameio-bridge.service", "Service systemd")
    ]
    
    results = []
    for item_path, description in required_items:
        path = Path(item_path)
        if path.exists():
            print(f"  ‚úÖ {description}: {item_path}")
            results.append(True)
        else:
            print(f"  ‚ùå {description}: {item_path} - Manquant")
            results.append(False)
    
    print(f"\nüìä R√©sultat: {sum(results)}/{len(results)} √©l√©ments pr√©sents")
    return all(results)

def test_documentation():
    """Test de la documentation"""
    print_section("DOCUMENTATION")
    
    docs_to_check = [
        ("docs/FRAMEIO_TESTING_GUIDE.md", "Guide de tests"),
        ("docs/FRAMEIO_LUCIDLINK_INTEGRATION.md", "Documentation int√©gration"),
        ("README_FRAMEIO_INTEGRATION.md", "README principal"),
        ("FRAMEIO_TESTING_SUMMARY.md", "R√©sum√© des tests")
    ]
    
    results = []
    for doc_path, description in docs_to_check:
        path = Path(doc_path)
        if path.exists():
            try:
                with open(path, 'r') as f:
                    content = f.read()
                    if len(content) > 100:  # Au moins 100 caract√®res
                        print(f"  ‚úÖ {description}: {len(content)} caract√®res")
                        results.append(True)
                    else:
                        print(f"  ‚ö†Ô∏è  {description}: Contenu trop court")
                        results.append(False)
            except Exception as e:
                print(f"  ‚ùå {description}: Erreur lecture - {e}")
                results.append(False)
        else:
            print(f"  ‚ùå {description}: Non trouv√©")
            results.append(False)
    
    print(f"\nüìä R√©sultat: {sum(results)}/{len(results)} documentations OK")
    return all(results)

def test_examples():
    """Test des exemples"""
    print_section("EXEMPLES")
    
    example_files = [
        ("examples/frameio_lucidlink_demo.py", "D√©mo principale"),
        ("examples/frameio_usage_examples.py", "Exemples d'usage"),
        ("examples/pipeline_demo.py", "D√©mo pipeline")
    ]
    
    results = []
    for example_path, description in example_files:
        path = Path(example_path)
        if path.exists():
            try:
                with open(path, 'r') as f:
                    content = f.read()
                    if 'def' in content or 'class' in content:
                        print(f"  ‚úÖ {description}: Code Python d√©tect√©")
                        results.append(True)
                    else:
                        print(f"  ‚ö†Ô∏è  {description}: Pas de code Python d√©tect√©")
                        results.append(False)
            except Exception as e:
                print(f"  ‚ùå {description}: Erreur lecture - {e}")
                results.append(False)
        else:
            print(f"  ‚ùå {description}: Non trouv√©")
            results.append(False)
    
    print(f"\nüìä R√©sultat: {sum(results)}/{len(results)} exemples OK")
    return all(results)

def generate_final_report():
    """G√©n√®re un rapport final"""
    print_section("RAPPORT FINAL")
    
    # Ex√©cuter tous les tests
    test_results = {
        "Structure des modules": test_module_structure(),
        "Parser complet": test_parser_comprehensive(),
        "Configuration": test_configuration_files(),
        "Compl√©tude du projet": test_project_completeness(),
        "Documentation": test_documentation(),
        "Exemples": test_examples()
    }
    
    # Calculer les statistiques
    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results.values() if result)
    success_rate = (passed_tests / total_tests) * 100
    
    print(f"\nüìä R√âSULTATS FINAUX:")
    print("-" * 30)
    
    for test_name, result in test_results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
    
    print("-" * 30)
    print(f"üìà Taux de r√©ussite: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
    
    if success_rate >= 90:
        print("üéâ EXCELLENT! L'int√©gration Frame.io est pr√™te pour la production.")
        status = "EXCELLENT"
    elif success_rate >= 75:
        print("‚úÖ BIEN! L'int√©gration Frame.io est fonctionnelle avec quelques am√©liorations possibles.")
        status = "BIEN"
    elif success_rate >= 60:
        print("‚ö†Ô∏è  ACCEPTABLE! L'int√©gration Frame.io fonctionne mais n√©cessite des corrections.")
        status = "ACCEPTABLE"
    else:
        print("‚ùå PROBL√âMATIQUE! L'int√©gration Frame.io n√©cessite des corrections importantes.")
        status = "PROBL√âMATIQUE"
    
    # Sauvegarder le rapport
    report = {
        "timestamp": datetime.now().isoformat(),
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "success_rate": success_rate,
        "status": status,
        "test_results": test_results
    }
    
    report_path = Path("output/frameio_validation_report.json")
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nüìÑ Rapport sauvegard√©: {report_path}")
    
    return success_rate >= 75

def main():
    """Fonction principale"""
    print_header("VALIDATION FINALE - INT√âGRATION FRAME.IO")
    
    print("üöÄ D√©but de la validation compl√®te...")
    print("Cette validation teste tous les composants essentiels")
    print("sans n√©cessiter de configuration externe.")
    
    success = generate_final_report()
    
    print_section("PROCHAINES √âTAPES")
    
    if success:
        print("‚úÖ L'int√©gration Frame.io est valid√©e!")
        print("\nüéØ Prochaines √©tapes recommand√©es:")
        print("1. Configurer les tokens Frame.io et Discord")
        print("2. Ex√©cuter les tests de connectivit√©:")
        print("   python scripts/test_frameio_interactive.py")
        print("3. Tester avec de vrais fichiers:")
        print("   python examples/frameio_lucidlink_demo.py")
        print("4. D√©ployer en production:")
        print("   bash scripts/deploy_frameio_integration.sh")
    else:
        print("‚ùå La validation a d√©tect√© des probl√®mes.")
        print("\nüîß Actions recommand√©es:")
        print("1. V√©rifier les erreurs ci-dessus")
        print("2. Corriger les fichiers manquants")
        print("3. Re-ex√©cuter la validation")
        print("4. Consulter la documentation:")
        print("   cat docs/FRAMEIO_TESTING_GUIDE.md")
    
    print(f"\nüìã Voir le r√©sum√© complet: FRAMEIO_TESTING_SUMMARY.md")
    print(f"üìä Rapport d√©taill√©: output/frameio_validation_report.json")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
