#!/usr/bin/env python3
"""
üöÄ PostFlow Runner - Bootstrap Module
====================================

Contient la boucle principale du pipeline et le workflow de traitement.
Extrait de main.py pour une meilleure organisation.

Version: 4.1.5
Date: 14 juillet 2025
"""

import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
from src.utils.config import ConfigManager

# Version du pipeline (synchronis√©e avec main.py)
POSTFLOW_VERSION = "4.1.5"
POSTFLOW_VERSION_NAME = "Emojis & Duplicate Detection Complete"

logger = logging.getLogger(__name__)

try:
    from src.utils.upload_tracker import UploadTracker
    from src.integrations.sheets.tracker import SheetsTracker
    from src.utils.thumbnail import ThumbnailGenerator
    from src.integrations.discord.notifier import DiscordNotifier
    from src.integrations.discord.user_notifier import DiscordUserNotifier
    from src.integrations.sheets.users import SheetsUserManager
    from src.utils.error_handler import ErrorHandler
    from src.utils.upload_queue import UploadQueue, QueuePriority
    PIPELINE_MODULES_AVAILABLE = True
except ImportError as e:
    logger.error(f"Pipeline modules not available: {e}")
    PIPELINE_MODULES_AVAILABLE = False

try:
    from src.utils.lucidlink_utils import lucidlink_waiter, lucidlink_detector
    LUCIDLINK_UTILS_AVAILABLE = True
except ImportError as e:
    logger.error(f"LucidLink utils not available: {e}")
    LUCIDLINK_UTILS_AVAILABLE = False


class PostFlowRunner:
    """Gestionnaire de la boucle principale du pipeline"""
    
    def __init__(self, config: Dict[str, Any], pipeline_config: Dict[str, Any], config_manager: ConfigManager):
        self.config = config
        self.pipeline_config = pipeline_config
        self.config_manager = config_manager
        
        # Composants du pipeline
        self.frameio_auth = None
        self.frameio_manager = None
        self.watcher = None
        self.dashboard_initializer = None
        self.infrastructure_manager = None
        self.error_handler = None
        
        # Int√©grations
        self.upload_tracker = None
        self.sheets_tracker = None
        self.thumbnail_generator = None
        self.discord_notifier = None
        self.user_notifier = None
        self.upload_queue = None
        
        # √âtat du pipeline
        self.is_running = False
        self._shutdown_event = asyncio.Event()
        self._event_loop = None
        
        # M√©triques
        self.metrics = {
            'start_time': datetime.now(),
            'files_processed': 0,
            'uploads_success': 0,
            'uploads_failed': 0,
            'token_refreshes': 0,
            'last_token_refresh': None,
            'last_activity': None
        }
        
        # Configuration de l'intervalle de v√©rification du token
        self.token_check_interval = pipeline_config.get('metrics', {}).get('token_check_interval', 300)
    
    def initialize_components(self, frameio_auth, frameio_manager, watcher, dashboard_initializer, 
                            infrastructure_manager, error_handler):
        """Initialise les composants du pipeline"""
        self.frameio_auth = frameio_auth
        self.frameio_manager = frameio_manager
        self.watcher = watcher
        self.dashboard_initializer = dashboard_initializer
        self.infrastructure_manager = infrastructure_manager
        self.error_handler = error_handler
        
        # Initialiser les int√©grations
        self._initialize_integrations()
    
    def _initialize_integrations(self):
        """Initialise les int√©grations (tracker, sheets, etc.)"""
        try:
            if not PIPELINE_MODULES_AVAILABLE:
                logger.warning("‚ö†Ô∏è Modules pipeline non disponibles")
                return
            
            logger.info("üîß Initialisation des int√©grations...")
            
            # Initialiser le tracker d'uploads
            self.upload_tracker = UploadTracker()
            
            # Initialiser le tracker Google Sheets avec connexions persistantes
            sheets_config = self.config.get('google_sheets', {})
            if sheets_config.get('enabled', True):
                try:
                    # Initialiser le gestionnaire de connexions Google persistantes
                    from src.integrations.google import connection_manager, OptimizedSheetsStatusAdapter
                    
                    credentials_file = sheets_config.get('service_account_file', 'config/google_credentials.json')
                    spreadsheet_id = sheets_config.get('spreadsheet_id')
                    
                    # Initialiser le gestionnaire avec les credentials
                    connection_manager.initialize(credentials_file, spreadsheet_id)
                    
                    # Ajouter le GoogleConnectionManager au config_manager pour les autres composants
                    self.config_manager.google_connection_manager = connection_manager
                    
                    # Tester la connexion
                    if connection_manager.test_connection(spreadsheet_id):
                        # Cr√©er l'adaptateur optimis√© pour Sheets
                        worksheet_name = sheets_config.get('worksheet_shots_tracks', 'SHOTS_TRACK')
                        self.sheets_adapter = OptimizedSheetsStatusAdapter(
                            connection_manager, 
                            spreadsheet_id, 
                            worksheet_name
                        )
                        
                        # Cr√©er le tracker avec l'adaptateur optimis√© et le GoogleConnectionManager
                        self.sheets_tracker = SheetsTracker(spreadsheet_id, user_manager=connection_manager)
                        self.sheets_tracker.status_adapter = self.sheets_adapter
                        
                        logger.info("‚úÖ Google Sheets tracker initialis√© avec connexions persistantes")
                        logger.info(f"üìä Stats connexions: {connection_manager.get_stats()}")
                    else:
                        logger.warning("‚ö†Ô∏è Test connexion Google Sheets √©chou√©, mode simulation")
                        self.sheets_tracker = SheetsTracker(spreadsheet_id)
                        self.sheets_adapter = None
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur initialisation connexions Google persistantes: {e}")
                    logger.warning("‚ö†Ô∏è Fallback vers mode simulation")
                    self.sheets_tracker = SheetsTracker(sheets_config.get('spreadsheet_id'))
                    self.sheets_adapter = None
            
            # Initialiser le g√©n√©rateur de thumbnails
            thumbnail_config = self.config.get('thumbnails', {})
            if thumbnail_config.get('enabled', True):
                self.thumbnail_generator = ThumbnailGenerator(self.config_manager)
            
            # Initialiser le notificateur Discord
            discord_config = self.config.get('discord', {})
            if discord_config.get('enabled', True):
                try:
                    # V√©rifier que l'URL webhook est pr√©sente
                    webhook_url = discord_config.get('webhook_url')
                    if not webhook_url or webhook_url == "YOUR_WEBHOOK_URL":
                        logger.warning("‚ö†Ô∏è Discord webhook URL non configur√©e, notifications d√©sactiv√©es")
                        self.discord_notifier = None
                    else:
                        # Cr√©er un wrapper de configuration pour DiscordNotifier
                        class DiscordConfigWrapper:
                            def __init__(self, discord_config):
                                self.discord_config = discord_config
                            
                            def get(self, key, default=None):
                                if key == 'discord.webhook_url':
                                    return self.discord_config.get('webhook_url', default)
                                elif key == 'discord.username':
                                    return self.discord_config.get('username', default)
                                elif key == 'discord.avatar_url':
                                    return self.discord_config.get('avatar_url', default)
                                return default
                        
                        config_wrapper = DiscordConfigWrapper(discord_config)
                        self.discord_notifier = DiscordNotifier(config_wrapper)
                        
                        # Initialiser le User Notifier avec int√©gration Google Sheets
                        try:
                            # Utiliser le user_manager existant du sheets_tracker si disponible
                            if self.sheets_tracker and hasattr(self.sheets_tracker, 'user_manager') and self.sheets_tracker.user_manager:
                                user_manager = self.sheets_tracker.user_manager
                                self.user_notifier = DiscordUserNotifier(
                                    discord_notifier=self.discord_notifier,
                                    user_manager=user_manager
                                )
                                logger.info("‚úÖ Discord User Notifier initialis√© avec int√©gration Google Sheets")
                            else:
                                logger.info("‚ÑπÔ∏è Pas d'user_manager disponible, utilisation du Discord Notifier standard")
                                self.user_notifier = None
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Impossible d'initialiser User Notifier: {e}")
                            logger.info("üîÑ Utilisation du Discord Notifier standard")
                            self.user_notifier = None
                        
                        logger.info("‚úÖ Discord notifier initialis√©")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur initialisation Discord: {e}")
                    self.discord_notifier = None
            
            # Initialiser la queue d'upload
            queue_config = self.pipeline_config.get('upload_queue', {})
            max_concurrent = queue_config.get('max_concurrent', 1)  # S√©quentiel par d√©faut
            max_queue_size = queue_config.get('max_queue_size', 100)
            
            self.upload_queue = UploadQueue(
                max_concurrent=max_concurrent,
                max_queue_size=max_queue_size
            )
            
            # D√©finir le callback de traitement
            self.upload_queue.set_process_callback(self._queue_file_processor)
            
            logger.info(f"üìã Queue d'upload initialis√©e (max_concurrent={max_concurrent}, max_queue_size={max_queue_size})")
            
            logger.info("‚úÖ Int√©grations initialis√©es avec succ√®s")
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur lors de l'initialisation des int√©grations: {e}")
    
    async def run_pipeline(self):
        f"""Lance le pipeline principal PostFlow v{POSTFLOW_VERSION}"""
        logger.info(f"üöÄ D√©marrage du pipeline PostFlow v{POSTFLOW_VERSION}...")
        
        # Stocker la boucle d'√©v√©nements pour les callbacks
        self._event_loop = asyncio.get_running_loop()
        
        # V√©rifier les composants essentiels
        if not self.frameio_manager:
            logger.error("[ERROR] Frame.io requis pour le fonctionnement")
            return False
        
        if not self.watcher:
            logger.error("[ERROR] Watcher requis pour le fonctionnement")
            return False
        
        self.is_running = True
        
        # V√©rification de synchronisation au d√©marrage
        logger.info("üîç Lancement de la v√©rification de synchronisation...")
        sync_ok = await self.startup_sync_check()
        if sync_ok:
            logger.info("‚úÖ V√©rification de synchronisation termin√©e")
        else:
            logger.warning("‚ö†Ô∏è V√©rification de synchronisation √©chou√©e")
        
        # Boucle principale
        try:
            logger.info(f"üîÑ Pipeline PostFlow v{POSTFLOW_VERSION} en cours d'ex√©cution...")
            
            # D√©marrer la queue d'upload
            if self.upload_queue:
                await self.upload_queue.start()
            
            # D√©marrer le watcher
            if self.watcher:
                logger.info("üëÅÔ∏è D√©marrage de la surveillance des fichiers...")
                self.watcher.start()
            
            # Notification de d√©marrage
            notifier = self.user_notifier or self.discord_notifier
            if notifier:
                await self._send_discord_notification(
                    f"üöÄ PostFlow v{POSTFLOW_VERSION} d√©marr√©",
                    f"Le pipeline de traitement {POSTFLOW_VERSION_NAME} est maintenant actif"
                )
            
            # Boucle principale avec arr√™t propre
            while self.is_running:
                try:
                    # V√©rifier et rafra√Æchir le token p√©riodiquement
                    if not await self._check_and_refresh_token():
                        logger.error("[ERROR] Probl√®me avec le token Frame.io")
                    
                    # Attendre soit le timeout soit l'√©v√©nement d'arr√™t
                    try:
                        await asyncio.wait_for(self._shutdown_event.wait(), timeout=self.token_check_interval)
                        # Si on arrive ici, l'√©v√©nement d'arr√™t a √©t√© d√©clench√©
                        logger.info("[STOP] √âv√©nement d'arr√™t d√©tect√©")
                        break
                    except asyncio.TimeoutError:
                        # Timeout normal, continuer la boucle
                        continue
                        
                except asyncio.CancelledError:
                    logger.info("[STOP] Boucle principale annul√©e")
                    break
                except Exception as e:
                    logger.error(f"[ERROR] Erreur dans la boucle principale: {e}")
                    # En cas d'erreur, attendre un peu avant de continuer
                    await asyncio.sleep(1)
                    
        except KeyboardInterrupt:
            logger.info("[STOP] Arr√™t demand√© par l'utilisateur")
        except Exception as e:
            logger.error(f"[ERROR] Erreur dans la boucle principale: {e}")
        finally:
            # Arr√™t avec timeout pour √©viter les blocages
            try:
                if self.is_running:
                    await asyncio.wait_for(self.shutdown(), timeout=30.0)
            except asyncio.TimeoutError:
                logger.warning("‚ö†Ô∏è Timeout lors de l'arr√™t, arr√™t forc√©")
                self.is_running = False
            except asyncio.CancelledError:
                logger.info("[STOP] Arr√™t annul√© - signal re√ßu")
                self.is_running = False
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur lors de l'arr√™t: {e}")
                self.is_running = False
        
        return True
    
    async def _handle_new_file(self, file_path: str, metadata: Dict[str, Any] = None, force: bool = False):
        """Callback pour traiter les nouveaux fichiers d√©tect√©s"""
        try:
            logger.info(f"üé¨ Nouveau fichier d√©tect√©: {file_path}")
            
            # Afficher les m√©tadonn√©es si disponibles
            if metadata:
                logger.info(f"üìä M√©tadonn√©es: {metadata}")
            
            # === V√âRIFICATION PR√âALABLE DES DOUBLONS ===
            if self.upload_tracker and not force:
                file_metadata = metadata or self._extract_metadata_from_path(Path(file_path))
                shot_id = file_metadata.get('shot_id', '')
                version = file_metadata.get('version', 'v001')
                
                duplicate = self.upload_tracker.is_duplicate(file_path, shot_id, version)
                if duplicate:
                    status = duplicate.get('status', 'UNKNOWN')
                    if status in ['COMPLETED', 'üéâ COMPLETED']:
                        logger.info(f"‚úÖ Fichier d√©j√† trait√©, ignor√©: {Path(file_path).name}")
                        return
                    elif status in ['CREATED', 'PROCESSING', 'REPROCESSING', '‚è≥ WAITING_APPROVAL', 'üîÑ UPLOADING', '‚öôÔ∏è PROCESSING', 'üîÑ REPROCESSING']:
                        logger.info(f"üîÑ Fichier d√©j√† en traitement, ignor√©: {Path(file_path).name}")
                        return
            
            # Ajouter le fichier √† la queue au lieu de le traiter directement
            if self.upload_queue:
                # D√©terminer la priorit√© bas√©e sur les m√©tadonn√©es
                priority = QueuePriority.NORMAL
                if metadata and metadata.get('urgent'):
                    priority = QueuePriority.HIGH
                elif force:
                    priority = QueuePriority.HIGH
                
                queue_id = await self.upload_queue.add_file(
                    Path(file_path),
                    priority=priority,
                    force=force,
                    metadata=metadata
                )
                
                logger.info(f"üìã Fichier ajout√© √† la queue: {Path(file_path).name} (ID: {queue_id})")
            else:
                # Fallback : traitement direct si pas de queue
                logger.warning("‚ö†Ô∏è Queue non disponible, traitement direct")
                await self._process_file_workflow(Path(file_path), force=force)
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur ajout fichier √† la queue {file_path}: {e}")
            # Fallback : essayer traitement direct
            try:
                await self._process_file_workflow(Path(file_path), force=force)
            except Exception as e2:
                logger.error(f"[ERROR] Erreur traitement direct {file_path}: {e2}")
    
    async def _process_file_workflow(self, file_path: Path, force: bool = False):
        """Workflow complet de traitement d'un fichier"""
        try:
            self.metrics['last_activity'] = datetime.now()
            
            # V√©rifier la stabilit√© du fichier
            if not await self._wait_for_file_stability(file_path):
                logger.error(f"‚ùå Fichier instable, abandon: {file_path}")
                return
            
            # === V√âRIFICATION DES DOUBLONS ===
            upload_id = None
            if self.upload_tracker:
                metadata = self._extract_metadata_from_path(file_path)
                shot_id = metadata.get('shot_id', '')
                version = metadata.get('version', 'v001')
                
                # V√©rifier si le fichier est d√©j√† trait√©
                duplicate = self.upload_tracker.is_duplicate(str(file_path), shot_id, version)
                if duplicate and not force:
                    status = duplicate.get('status', 'UNKNOWN')
                    if status in ['COMPLETED', 'üéâ COMPLETED']:
                        logger.info(f"‚úÖ Fichier d√©j√† trait√© avec succ√®s: {file_path.name}")
                        logger.info(f"üìã Upload ID existant: {duplicate.get('upload_id', 'N/A')}")
                        return duplicate.get('upload_id')
                    elif status in ['CREATED', 'PROCESSING', '‚è≥ WAITING_APPROVAL', 'üîÑ UPLOADING', '‚öôÔ∏è PROCESSING']:
                        logger.info(f"üîÑ Fichier en cours de traitement: {file_path.name}")
                        logger.info(f"üìã Upload ID existant: {duplicate.get('upload_id', 'N/A')}")
                        return duplicate.get('upload_id')
                    else:
                        logger.warning(f"‚ö†Ô∏è Fichier pr√©c√©demment √©chou√©, nouveau traitement: {file_path.name}")
                
                # Ajouter nouveau tracking ou r√©cup√©rer l'existant
                if duplicate and force:
                    upload_id = duplicate.get('upload_id')
                    logger.info(f"üîÑ Retraitement forc√©: {file_path.name} (ID: {upload_id})")
                    # Mettre √† jour le statut pour indiquer un nouveau traitement
                    self.upload_tracker.update_upload(upload_id, {
                        'status': 'üîÑ REPROCESSING',
                        'reprocessing_at': datetime.now().isoformat()
                    })
                else:
                    upload_id = self.upload_tracker.add_upload(
                        str(file_path),
                        shot_id,
                        version,
                        metadata
                    )
                    logger.info(f"üìã Nouveau tracking cr√©√©: {file_path.name} (ID: {upload_id})")
            
            # 1. G√©n√©rer la miniature avec upload vers Google Drive pour Discord
            thumbnail_path = None
            thumbnail_url = None
            if self.config.get('workflow', {}).get('enable_thumbnails', True):
                metadata = self._extract_metadata_from_path(file_path)
                shot_name = metadata.get('shot_name', '')
                
                # G√©n√©rer et uploader directement depuis la vid√©o vers Hostinger
                if self.thumbnail_generator:
                    thumbnail_url = self.thumbnail_generator.generate_with_hostinger_upload(
                        str(file_path), shot_name
                    )
            
            # 2. V√©rifier et rafra√Æchir le token si n√©cessaire
            if not await self._check_and_refresh_token():
                logger.error("[ERROR] Token Frame.io invalide")
                return
            
            # 3. Upload vers Frame.io avec la nouvelle m√©thode
            frameio_link = None
            if self.config.get('workflow', {}).get('enable_frameio_upload', True):
                try:
                    frameio_link = await self._upload_to_frameio(file_path)
                except Exception as e:
                    logger.error(f"[ERROR] Erreur upload Frame.io: {e}")
            
            # 4. Upload thumbnail vers Google Drive et mise √† jour Google Sheets
            thumbnail_drive_url = thumbnail_url  # D√©j√† upload√© √† l'√©tape 1
            
            # 5. Mise √† jour Google Sheets avec les infos du traitement
            if self.config.get('workflow', {}).get('enable_sheets_updates', True):
                try:
                    metadata = self._extract_metadata_from_path(file_path)
                    await self._update_sheets_with_processing_info(
                        metadata, frameio_link, thumbnail_drive_url, file_path
                    )
                except Exception as e:
                    logger.error(f"[ERROR] Erreur mise √† jour Google Sheets: {e}")
            
            # 6. Notification Discord avec thumbnail
            if self.config.get('workflow', {}).get('enable_discord_notifications', True):
                try:
                    await self._send_file_notification(file_path, frameio_link, thumbnail_url)
                except Exception as e:
                    logger.error(f"[ERROR] Erreur notification Discord: {e}")
            
            # Marquer comme approuv√© automatiquement pour l'instant (peut √™tre modifi√© pour workflow d'approbation)
            if upload_id and self.upload_tracker:
                self.upload_tracker.approve_upload(upload_id, "auto-approved")
                self.upload_tracker.start_upload(upload_id)
            
            # Mettre √† jour les m√©triques
            self.metrics['files_processed'] += 1
            
            # === R√âSUM√â DU TRACKING ===
            if upload_id and self.upload_tracker:
                try:
                    self.upload_tracker.update_upload(upload_id, {
                        'frameio_link': frameio_link,
                        'thumbnail_url': thumbnail_drive_url,
                        'processing_time': datetime.now().isoformat(),
                        'status': 'üéâ COMPLETED'
                    })
                except Exception as e:
                    logger.error(f"[ERROR] Erreur finalisation tracking: {e}")
            
            logger.info(f"[PARTY] Workflow termin√© avec succ√®s: {file_path.name}")
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur workflow: {e}")
            self.metrics['uploads_failed'] += 1
            
            # Mettre √† jour le tracker avec l'erreur
            if upload_id and self.upload_tracker:
                try:
                    self.upload_tracker.update_upload(upload_id, {
                        'status': '‚ùå FAILED',
                        'error': str(e),
                        'processing_time': datetime.now().isoformat()
                    })
                except Exception as track_error:
                    logger.error(f"[ERROR] Erreur tracking √©chec: {track_error}")
    
    async def _generate_thumbnail(self, file_path: Path) -> tuple[Optional[str], Optional[str]]:
        """G√©n√®re une thumbnail pour le fichier"""
        try:
            if not self.thumbnail_generator:
                return None, None
            
            thumbnail_path = self.thumbnail_generator.generate(str(file_path))
            
            # Upload vers le serveur HTTP pour URL temporaire
            if thumbnail_path and self.infrastructure_manager and self.infrastructure_manager.get_http_server():
                http_server = self.infrastructure_manager.get_http_server()
                thumbnail_url = http_server.serve_file(thumbnail_path)  # serve_file est synchrone
                return thumbnail_path, thumbnail_url
            
            return thumbnail_path, None
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur g√©n√©ration thumbnail: {e}")
            return None, None
    
    async def _upload_to_frameio(self, file_path: Path) -> Optional[str]:
        """Upload le fichier vers Frame.io"""
        try:
            if not self.frameio_manager:
                return None
            
            # Extraire les m√©tadonn√©es pour obtenir shot_name et scene_name
            metadata = self._extract_metadata_from_path(file_path)
            shot_name = metadata.get('shot_id', '')
            scene_name = metadata.get('scene_name', '')
            
            if not shot_name:
                logger.error(f"[ERROR] Impossible d'extraire shot_name de: {file_path}")
                return None
            
            # Utiliser la m√©thode de production avec remote_upload via Cloudflare/RangeServer
            result = await self.frameio_manager.upload_file_production(
                file_path=str(file_path),
                shot_name=shot_name,
                scene_name=scene_name
            )
            
            if result:
                self.metrics['uploads_success'] += 1
                logger.info(f"‚úÖ Upload Frame.io r√©ussi: {result}")
                return result
            else:
                self.metrics['uploads_failed'] += 1
                logger.error(f"[ERROR] Upload Frame.io √©chou√© pour: {file_path.name}")
                return None
            
        except Exception as e:
            self.metrics['uploads_failed'] += 1
            logger.error(f"[ERROR] Erreur upload Frame.io: {e}")
            return None
    
    async def _check_and_refresh_token(self) -> bool:
        """V√©rifie et rafra√Æchit le token Frame.io si n√©cessaire"""
        try:
            if not self.frameio_auth:
                return False
            
            # V√©rifier si le token existe et est valide
            if await self.frameio_auth.test_connection():
                return True
            
            # Tenter de rafra√Æchir le token
            if await self.frameio_auth.refresh_token():
                self.metrics['token_refreshes'] += 1
                self.metrics['last_token_refresh'] = datetime.now()
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur v√©rification token: {e}")
            return False
    
    async def _wait_for_file_stability(self, file_path: Path, max_wait: int = 60, check_interval: int = 2) -> bool:
        """Attend que le fichier soit stable avec support LucidLink am√©lior√©"""
        try:
            if not file_path.exists():
                logger.warning(f"‚ö†Ô∏è Fichier non trouv√©: {file_path}")
                return False
            
            # Utiliser les utilitaires LucidLink si disponibles
            if LUCIDLINK_UTILS_AVAILABLE:
                logger.info(f"üîç V√©rification LucidLink pour: {file_path.name}")
                
                # V√©rifier si c'est un fichier LucidLink
                if lucidlink_detector.is_lucidlink_file(file_path):
                    logger.info(f"üîÑ Fichier LucidLink d√©tect√©: {file_path.name}")
                    
                    # Utiliser l'attente sp√©cialis√©e pour LucidLink (max 5 minutes)
                    return await lucidlink_waiter.wait_for_complete_sync(
                        file_path, 
                        max_wait=max(max_wait, 300),
                        check_interval=check_interval
                    )
                else:
                    logger.debug(f"üìÅ Fichier local standard: {file_path.name}")
                    return await lucidlink_waiter._standard_stability_check(
                        file_path, max_wait, check_interval
                    )
            else:
                logger.warning("‚ö†Ô∏è Utilitaires LucidLink non disponibles, utilisation m√©thode standard")
                return await self._standard_stability_check(file_path, max_wait, check_interval)
                
        except Exception as e:
            logger.error(f"[ERROR] Erreur v√©rification stabilit√©: {file_path} - {e}")
            return False
    
    async def _standard_stability_check(self, file_path: Path, max_wait: int = 60, check_interval: int = 2) -> bool:
        """V√©rification de stabilit√© standard (fallback)"""
        try:
            if not file_path.exists():
                logger.warning(f"‚ö†Ô∏è Fichier non trouv√©: {file_path}")
                return False
            
            logger.info(f"‚è≥ V√©rification stabilit√© standard: {file_path.name}")
            
            prev_size = None
            prev_mtime = None
            stable_checks = 0
            required_stable_checks = 3
            
            start_time = asyncio.get_event_loop().time()
            
            while (asyncio.get_event_loop().time() - start_time) < max_wait:
                try:
                    stat = file_path.stat()
                    current_size = stat.st_size
                    current_mtime = stat.st_mtime
                    
                    if prev_size is not None and prev_mtime is not None:
                        if current_size == prev_size and current_mtime == prev_mtime:
                            stable_checks += 1
                            if stable_checks >= required_stable_checks:
                                logger.info(f"‚úÖ Fichier stable: {file_path.name}")
                                return True
                        else:
                            stable_checks = 0
                    
                    prev_size = current_size
                    prev_mtime = current_mtime
                    
                    await asyncio.sleep(check_interval)
                    
                except (OSError, PermissionError) as e:
                    logger.warning(f"‚ö†Ô∏è Erreur acc√®s fichier: {file_path} - {e}")
                    await asyncio.sleep(check_interval)
                    continue
            
            logger.warning(f"‚è∞ Timeout stabilit√© standard: {file_path}")
            return False
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur v√©rification stabilit√© standard: {file_path} - {e}")
            return False
    
    def _extract_metadata_from_path(self, file_path: Path) -> Dict[str, str]:
        """Extrait les m√©tadonn√©es √† partir du chemin du fichier"""
        try:
            path_parts = file_path.parts
            file_name = file_path.name
            
            metadata = {}
            
            # Utiliser la validation stricte pour extraire les donn√©es
            try:
                from src.integrations.frameio.upload import validate_strict_nomenclature
                nomenclature_data = validate_strict_nomenclature(str(file_path))
                
                # Copier toutes les donn√©es valid√©es
                metadata.update(nomenclature_data)
                
                # Ajouter quelques mappings pour compatibilit√©
                metadata['shot_name'] = nomenclature_data.get('shot_id', '')
                metadata['nomenclature'] = nomenclature_data.get('shot_id', '')
                
                logger.info(f"‚úÖ M√©tadonn√©es extraites avec validation stricte")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Validation stricte √©chou√©e, fallback sur regex: {e}")
                
                # Fallback sur l'ancienne m√©thode si la validation √©choue
                if len(path_parts) >= 3:
                    # Trouver la sc√®ne dans le chemin
                    for part in path_parts:
                        if part.startswith('SQ'):
                            metadata['scene_name'] = part
                            break
                    
                    # Extraire du nom de fichier
                    if 'UNDLM_' in file_name:
                        parts = file_name.split('_')
                        if len(parts) >= 4:
                            metadata['shot_name'] = f"{parts[0]}_{parts[1]}_{parts[2]}"
                            metadata['nomenclature'] = metadata['shot_name']
                            metadata['version'] = parts[3].split('.')[0]
            
            # Ajouter la taille du fichier
            try:
                metadata['file_size'] = file_path.stat().st_size
            except:
                metadata['file_size'] = 0
            
            # Debug: afficher les m√©tadonn√©es extraites
            logger.info(f"üìä M√©tadonn√©es extraites: {metadata}")
            
            return metadata
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur extraction m√©tadonn√©es: {e}")
            return {}
    
    async def _upload_thumbnail_to_drive(self, thumbnail_path: str, shot_name: str, nomenclature: str) -> Optional[str]:
        """Upload une thumbnail vers Google Drive et retourne l'URL pour la formule =IMAGE()"""
        try:
            if not self.thumbnail_generator:
                logger.warning("‚ö†Ô∏è Thumbnail generator non disponible")
                return None
            
            # Utiliser la m√©thode Hostinger int√©gr√©e du ThumbnailGenerator
            drive_url = self.thumbnail_generator.generate_with_hostinger_upload(
                thumbnail_path, shot_name
            )
            
            if drive_url:
                logger.info(f"‚òÅÔ∏è Thumbnail upload√© vers Google Drive: {shot_name}")
                return drive_url
            else:
                logger.warning(f"‚ö†Ô∏è √âchec upload thumbnail Drive: {shot_name}")
                return None
                
        except Exception as e:
            logger.error(f"[ERROR] Erreur upload thumbnail vers Drive: {e}")
            return None
    
    async def _update_sheets_with_processing_info(self, metadata: dict, frameio_link: str = None, 
                                                thumbnail_drive_url: str = None, file_path: Path = None):
        """Met √† jour Google Sheets avec les informations de traitement - Version optimis√©e"""
        try:
            if not self.sheets_adapter:
                logger.warning("‚ö†Ô∏è Adaptateur Google Sheets non disponible")
                return
            
            nomenclature = metadata.get('nomenclature', '')
            if not nomenclature:
                logger.error("[ERROR] Pas de nomenclature disponible pour Google Sheets")
                return
            
            # Trouver la ligne du shot
            row_number = await self.sheets_adapter.find_shot_row(nomenclature)
            if not row_number:
                logger.warning(f"‚ö†Ô∏è Shot non trouv√© dans Google Sheets: {nomenclature}")
                return
            
            # Pr√©parer les donn√©es de mise √† jour avec formules Google Sheets
            update_data = {
                'version': metadata.get('version', ''),
                'file_name': metadata.get('filename', file_path.name if file_path else ''),
                'processing_date': datetime.now().isoformat(),
            }
            
            # Frame.io link avec formule LIEN_HYPERTEXTE
            if frameio_link:
                version = metadata.get('version', 'v001')
                link_text = f"REVIEW {version}"
                frameio_formula = f'=LIEN_HYPERTEXTE("{frameio_link}";"{link_text}")'
                update_data['frameio_link'] = frameio_formula
                logger.info(f"üìä Formule Frame.io pr√©par√©e: {link_text}")
            
            # Thumbnail avec formule =IMAGE()
            if thumbnail_drive_url:
                update_data['thumbnail_url'] = thumbnail_drive_url  # L'adaptateur g√®re la formule IMAGE()
                logger.info(f"üñºÔ∏è Formule thumbnail pr√©par√©e")
            
            # Mettre √† jour avec l'adaptateur optimis√© (connexion persistante)
            success = await self.sheets_adapter.update_status(
                row_number=row_number,
                status="PROCESSED", 
                additional_data=update_data
            )
            
            if success:
                logger.info(f"‚úÖ Google Sheets mis √† jour avec formules pour: {nomenclature}")
            else:
                logger.error(f"‚ùå √âchec mise √† jour Google Sheets pour: {nomenclature}")
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur mise √† jour Sheets optimis√©e: {e}")
    
    async def _send_file_notification(self, file_path: Path, frameio_link: str = None, thumbnail_url: str = None):
        """Envoie une notification Discord pour un fichier trait√©"""
        try:
            # Utiliser user_notifier en priorit√©, sinon discord_notifier
            notifier = self.user_notifier or self.discord_notifier
            if not notifier:
                return
            
            message = f"üé¨ Fichier trait√©: {file_path.name}"
            if frameio_link:
                message += f"\n[LINK] Frame.io: {frameio_link}"
            
            # Si on utilise le user_notifier
            if self.user_notifier:
                result = await self.user_notifier.notify_file_processed(
                    file_path=file_path,
                    frameio_link=frameio_link,
                    thumbnail_url=thumbnail_url
                )
            else:
                # Fallback vers discord_notifier classique
                result = self.discord_notifier.notify_file_processed(
                    file_path.name, message, frameio_link, thumbnail_url
                )
            
            if not result:
                logger.debug(f"‚ö†Ô∏è Notification fichier Discord non envoy√©e: {file_path.name}")
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur notification Discord: {e}")
    
    async def _send_discord_notification(self, title: str, message: str):
        """Envoie une notification Discord de fa√ßon asynchrone"""
        try:
            # Utiliser user_notifier en priorit√©, sinon discord_notifier
            notifier = self.user_notifier or self.discord_notifier
            if not notifier:
                return
            
            # Si on utilise le user_notifier
            if self.user_notifier:
                result = await self.user_notifier.send_system_notification(title, message)
            else:
                # Fallback vers discord_notifier classique
                result = self.discord_notifier.notify_system_status(title, message)
            
            if not result:
                logger.debug(f"‚ö†Ô∏è Notification Discord non envoy√©e: {title}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur notification Discord: {e}")
    
    async def startup_sync_check(self) -> bool:
        """V√©rification de synchronisation au d√©marrage du pipeline"""
        try:
            from src.bootstrap.sync_checker import SyncChecker
            
            # Cr√©er et utiliser le vrai SyncChecker
            checker = SyncChecker(
                self.config, 
                self.config_manager, 
                self.upload_tracker,
                self.discord_notifier or self.user_notifier
            )
            
            # Lancer la v√©rification avec callback et sans limitation de fichiers
            return await checker.startup_sync_check(
                process_file_callback=self._handle_new_file,
                max_files_to_process=999  # Traiter tous les fichiers
            )
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur v√©rification synchronisation: {e}")
            return False
    
    async def shutdown(self):
        f"""Arr√™t propre du pipeline PostFlow v{POSTFLOW_VERSION}"""
        if not self.is_running:
            return
            
        logger.info(f"[STOP] Arr√™t du pipeline PostFlow v{POSTFLOW_VERSION}...")
        
        self.is_running = False
        
        # D√©clencher l'√©v√©nement d'arr√™t pour sortir de la boucle principale
        self._shutdown_event.set()
        
        # Notification d'arr√™t
        notifier = self.user_notifier or self.discord_notifier
        if notifier:
            try:
                await asyncio.wait_for(
                    self._send_discord_notification(
                        f"[STOP] PostFlow v{POSTFLOW_VERSION} arr√™t√©",
                        "Le pipeline de traitement a √©t√© arr√™t√©"
                    ),
                    timeout=5.0
                )
            except asyncio.TimeoutError:
                logger.warning("‚ö†Ô∏è Timeout notification Discord")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur notification Discord: {e}")
        
        # Arr√™ter le watcher
        if self.watcher:
            logger.info("[STOP] Arr√™t du watcher...")
            try:
                self.watcher.stop()
                logger.info("‚úÖ Watcher arr√™t√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur arr√™t watcher: {e}")
        
        # Arr√™ter la queue d'upload
        if self.upload_queue:
            logger.info("[STOP] Arr√™t de la queue d'upload...")
            try:
                await self.upload_queue.stop()
                logger.info("‚úÖ Queue d'upload arr√™t√©e")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur arr√™t queue d'upload: {e}")
        
        # Arr√™ter l'infrastructure
        if self.infrastructure_manager:
            self.infrastructure_manager.stop_infrastructure()
        
        # Arr√™ter le dashboard
        if self.dashboard_initializer:
            self.dashboard_initializer.stop_dashboard()
        
        # Annuler toutes les t√¢ches asyncio en cours
        try:
            current_task = asyncio.current_task()
            tasks = [task for task in asyncio.all_tasks() if not task.done() and task != current_task]
            if tasks:
                logger.info(f"[STOP] Annulation de {len(tasks)} t√¢ches asyncio...")
                for task in tasks:
                    if not task.done():
                        task.cancel()
                
                # Attendre que toutes les t√¢ches soient annul√©es (avec timeout)
                if tasks:
                    await asyncio.wait(tasks, timeout=5.0, return_when=asyncio.ALL_COMPLETED)
                logger.info("‚úÖ T√¢ches asyncio annul√©es")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur annulation t√¢ches: {e}")
        
        # Afficher les m√©triques finales
        duration = datetime.now() - self.metrics['start_time']
        print("\nüìä M√âTRIQUES FINALES")
        print("-" * 50)
        print(f"Dur√©e d'ex√©cution: {duration}")
        print(f"Fichiers trait√©s: {self.metrics['files_processed']}")
        print(f"Uploads r√©ussis: {self.metrics['uploads_success']}")
        print(f"Uploads √©chou√©s: {self.metrics['uploads_failed']}")
        print(f"Rafra√Æchissements token: {self.metrics['token_refreshes']}")
        if self.metrics['last_token_refresh']:
            print(f"Dernier rafra√Æchissement: {self.metrics['last_token_refresh']}")
        
        logger.info(f"‚úÖ Pipeline PostFlow v{POSTFLOW_VERSION} arr√™t√©")
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut du pipeline pour le dashboard"""
        status = {
            'running': self.is_running,
            'components': {
                'frameio': self.frameio_manager is not None,
                'watcher': self.watcher is not None,
                'error_handler': self.error_handler is not None,
                'dashboard': self.dashboard_initializer is not None,
                'upload_queue': self.upload_queue is not None
            },
            'metrics': self.metrics,
            'config': {
                'frameio_enabled': self.frameio_manager is not None,
                'watcher_enabled': self.watcher is not None,
                'error_handler_enabled': self.error_handler is not None,
                'upload_queue_enabled': self.upload_queue is not None
            }
        }
        
        # Ajouter les informations de la queue si disponible
        if self.upload_queue:
            status['upload_queue'] = self.upload_queue.get_status()
        
        return status
    
    def force_shutdown(self):
        """Arr√™t forc√© pour debugging - ne pas utiliser en production"""
        logger.warning("‚ö†Ô∏è Arr√™t forc√© demand√© (mode debug)")
        self.is_running = False
        self._shutdown_event.set()
    
    async def process_file(self, file_path: Path, force: bool = False) -> bool:
        """Traite un fichier sp√©cifique via la queue"""
        try:
            if self.upload_queue:
                # Traitement via la queue avec priorit√© haute
                queue_id = await self.upload_queue.add_file(
                    file_path,
                    priority=QueuePriority.HIGH,
                    force=force,
                    metadata={'source': 'manual_processing'}
                )
                
                logger.info(f"üìã Fichier ajout√© √† la queue pour traitement: {file_path.name} (ID: {queue_id})")
                
                # Attendre que le fichier soit trait√©
                timeout = 300  # 5 minutes max
                start_time = datetime.now()
                
                while (datetime.now() - start_time).total_seconds() < timeout:
                    item_status = self.upload_queue.get_item_status(queue_id)
                    if not item_status:
                        break
                    
                    if item_status['status'] == 'COMPLETED':
                        logger.info(f"‚úÖ Traitement termin√©: {file_path.name}")
                        return True
                    elif item_status['status'] == 'FAILED':
                        logger.error(f"[ERROR] Traitement √©chou√©: {file_path.name}")
                        return False
                    
                    await asyncio.sleep(1)
                
                logger.warning(f"‚ö†Ô∏è Timeout traitement: {file_path.name}")
                return False
            else:
                # Traitement direct si pas de queue
                await self._process_file_workflow(file_path, force)
                return True
                
        except Exception as e:
            logger.error(f"[ERROR] Erreur traitement fichier: {e}")
            return False
    
    async def _queue_file_processor(self, file_path: str, metadata: Dict[str, Any] = None, force: bool = False):
        """Wrapper pour traiter les fichiers via la queue (adaptation des signatures)"""
        try:
            logger.info(f"üìã [Queue] Traitement: {Path(file_path).name}")
            
            # Convertir le string en Path et appeler le workflow
            await self._process_file_workflow(Path(file_path), force=force)
            
        except Exception as e:
            logger.error(f"[ERROR] [Queue] Erreur traitement {file_path}: {e}")
            raise  # Re-raise pour que la queue g√®re le retry
